# Mixed Provider Configuration with Smart Context Routing

[server]
port = 3456
host = "127.0.0.1"
log_level = "info"

# Local model for most tasks
[[providers]]
name = "ollama"
api_base_url = "http://localhost:11434/v1/chat/completions"
api_key = "ollama"
models = ["qwen2.5-coder:latest"]

[providers.model_context]
"qwen2.5-coder:latest" = 32768

# Cloud model for complex reasoning
[[providers]]
name = "deepseek"
api_base_url = "https://api.deepseek.com/chat/completions"
api_key = "$DEEPSEEK_API_KEY"
models = ["deepseek-chat", "deepseek-reasoner"]

[providers.transformers]
use = ["deepseek"]

[providers.model_context]
"deepseek-chat" = 64000
"deepseek-reasoner" = 64000

# Cloud model for large context
[[providers]]
name = "openrouter"
api_base_url = "https://openrouter.ai/api/v1/chat/completions"
api_key = "$OPENROUTER_API_KEY"
models = ["google/gemini-2.0-flash-exp"]

[providers.model_context]
"google/gemini-2.0-flash-exp" = 1000000

[router]
default = "ollama.qwen2.5-coder:latest"
background = "ollama.qwen2.5-coder:latest"
think = "deepseek.deepseek-reasoner"

# Optional: Long context routing for large codebases
longContext = "openrouter.google/gemini-2.0-flash-exp"
longContextThreshold = 60000                     # Switch when >60K tokens
